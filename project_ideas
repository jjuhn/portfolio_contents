1. https://www.quora.com/What-are-some-artificial-neural-network-project-ideas-for-undergraduates 
    - Korean website: https://www.edwith.org/deeplearningchoi/ 
    - https://github.com/zzsza/Deep_Learning_starting_with_the_latest_papers
    
2. Bayesian modeling tutorial?
    
    - Dependent vs Independent sampling
        - independent sampling: limited to special cases, effective only in low dimensions, require special tailoring for efficiency(time consuming)
        - dependent sampling: wider variety of simulation methods, complex, high dimensional but requires more samples for a given accuracy
        - The Monte Carlo standard error (SE) formula for independent sampling is NOT appropriate for simulation methods based on dependent sampling.
        - A sequence of independent random variables CAN be called a Markov chain
        
    - Markov chain explanation(what is the purpose? where it is used?
    - MCMC explanation
        - Good paper: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.13.7133&rep=rep1&type=pd
        - make one in python and one for R. 
            - python: https://twiecki.github.io/blog/2015/11/10/mcmc-sampling/
            - R: I know how to do it. hubble telescope data. 
        - Gibbs Sampling
        - Metropolis algorithms (simplify with example) 
        - Metropolis Hastings
    - Practical MCMC in R.
        - JAGS(DAGS)
        - Decisions: how many chains?, starting point?, converged?, transient?, which iterates to use?, how many iterates do we need?
            - run several independent chains from different sparse starting points(3 to 5 chains)
            - overdispersed starting point is much better: multiple modes detection is easier, so as convergence diagnostics
            - in hierarchical model, only the top level hyperparameter should be initialized. starting values that are too extreme may cause software to crash
            - automatic tuning: **Adaptation**, use the tuning but discard the initial run 
            - convergence: the good chain = posterior is the limiting distribution
3. Check out datastudio by google for visualization project 
    - what to visualize? maybe some sort of data science related algorithms. apriori?
4. Linear Regression using R (DONE, house price prediction project)
5. Linear Regression using R in MCMC approach. 
6. What is the difference? Which is better? 
7. About Kaggle Competition: 
     - Algorithms that are often used 
        - XGBoost
        - LightGBM
        - Catboost
        - StackNet
     - HOW TO WIN IN KAGGLE:  https://www.coursera.org/learn/competitive-data-science/  
     - PyCon Korea:  https://www.slideshare.net/yeonminkim/pycon-korea-2018-kaggle-tutorialkaggle-break 
8. List of possible interview questions
9. Personal Diary about my gap year: study, things I did, parenting etc.
10. Recent Paper Reviews:
    - Using Bayesian Aldrich-Mckelvey Scaling to study citizens' ideological preferences and perceptions
11. Fix the current portfolio website
    - R markdown/ ipython notebook incorporation
12. Prioritize this list 
13. Teaching materials: https://intellipaat.com/data-scientist-course-training/?utm_source=Quora&utm_medium=Paid&utm_campaign=Data%20Science-US-Topic-Image#about-course
14. Everybody lies : book review and learn about the big data 
15. Automated Vehicle research
16. movie recommendation: 
    - instead of using the curated movie db, try and get sentimental analysis on twitter, imdb, reddit.
    - take user input such as age, gender, occupation( use user cached data? or user cookies to figure out?).  Need solution for the fresh start user problem on the recommender system. 
17. Data mining: Paradigmatic vs Syntagmatic relationship.  
    - Paradigmatic: replaceable words, high context similarities. cat and dog vs cat and computer
    - Syntagmatic: put verb in between and see. Correlated occurence. They can be combined with each other to convey meaning 
    - EOWC: Expected Overlap of Words in Context in Paradigmatic relationship.
        - Two problems
            - It favors matching one frequent term very well over matching more distinct terms
            - It treats every word equally
        - Solutions
            - Use Term Frequencies instead of Count. 
            - Use IDF to differentiate stop words from rare words. 
        - BM25 Example
    - Entropy: Measure of Randomness. H(x) 
        - higher the more random.
        - Conditional Entropy: H(x|y) should be less random than H(x) if it is same as H(x) then x and y are independent
        
    - Information Theory: mutual information I(X;Y) 
        
    
     
   




        
        
